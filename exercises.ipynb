{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b53e8b",
   "metadata": {},
   "source": [
    "# Pandas Exercises\n",
    "\n",
    "## Overview\n",
    "These exercises use realistic sales data with intentional \"mess\". \n",
    "You'll work with three CSV files in the **/files** folder:\n",
    "- **products_master.csv** (8 rows): Product catalog with pricing\n",
    "- **sales_records.csv** (260 rows): Transaction data\n",
    "- **regions_dim.csv** (5 rows): Region code mappings\n",
    "\n",
    "### Issues With Data (Which Need Solving)\n",
    "- Some products have **no sales**\n",
    "- Some sales reference **unknown products**\n",
    "- Some regions exist only in one but **not** the other table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd9e86",
   "metadata": {},
   "source": [
    "## Exercise 1: Basic Filtering & Boolean Logic\n",
    "**Goal**: Filter sales data using multiple conditions\n",
    "\n",
    "### Tasks:\n",
    "1. Find all sales in AMER or EMEA regions\n",
    "2. Filter for Plus (P-PLS) or Pro (P-PRO) products only\n",
    "3. Show transactions with 20 or more units\n",
    "4. Combine all three filters: AMER/EMEA + Plus/Pro + units ≥ 20\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Multiple conditions with &, |, and parentheses\n",
    "df[(df['region_code'].isin(['AMER', 'EMEA'])) & (df['units'] >= 20)]\n",
    "\n",
    "# OR logic for products\n",
    "df[df['product_id'].isin(['P-PLS', 'P-PRO'])]\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- Task 1: ~130-150 rows\n",
    "- Task 4: ~10-15 rows (highly filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe945e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c2d16",
   "metadata": {},
   "source": [
    "## Exercise 2: Aggregation & Grouping\n",
    "**Goal**: Summarize sales by different dimensions\n",
    "\n",
    "### Tasks:\n",
    "1. Calculate total revenue and units sold by product_id\n",
    "2. Find average unit_price per region_code\n",
    "3. Create a summary table: product_id × region_code with total revenue\n",
    "4. Extract year-month from order_date and sum revenue by month\n",
    "5. Find the top 3 best-selling products (by units) in each region\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Basic groupby\n",
    "df.groupby('product_id')['revenue'].sum()\n",
    "\n",
    "# Multiple aggregations\n",
    "df.groupby('region_code').agg({\n",
    "    'revenue': 'sum',\n",
    "    'units': 'mean'\n",
    "})\n",
    "\n",
    "# Year-month extraction\n",
    "df['year_month'] = pd.to_datetime(df['order_date']).dt.to_period('M')\n",
    "\n",
    "# Top N per group\n",
    "df.groupby('region_code').apply(\n",
    "    lambda x: x.nlargest(3, 'units')\n",
    ").reset_index(drop=True)\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- Task 1: 8-9 products (some in sales but not in master)\n",
    "- Task 3: ~20-30 product-region combinations\n",
    "- Task 5: 15 rows (3 products × 5 regions, though some regions may have fewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba48e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a9df7",
   "metadata": {},
   "source": [
    "## Exercise 3: Date Parsing & Time-Based Analysis\n",
    "**Goal**: Work with date columns and extract time components\n",
    "\n",
    "### Tasks:\n",
    "1. Convert order_date to datetime format (it's already clean, use `pd.to_datetime()`)\n",
    "2. Extract year, month, quarter, and day_of_week from order_date\n",
    "3. Create a new column for \"fiscal_quarter\" (assuming fiscal year starts in April)\n",
    "4. Calculate days_since_order (from October 22, 2025)\n",
    "5. Group sales by quarter and calculate total revenue\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Parse dates\n",
    "df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "\n",
    "# Extract components\n",
    "df['year'] = df['order_date'].dt.year\n",
    "df['month'] = df['order_date'].dt.month\n",
    "df['quarter'] = df['order_date'].dt.quarter\n",
    "df['weekday'] = df['order_date'].dt.day_name()\n",
    "\n",
    "# Fiscal quarter (Q1 = Apr-Jun, Q2 = Jul-Sep, etc.)\n",
    "df['fiscal_quarter'] = ((df['order_date'].dt.month - 4) % 12 // 3) + 1\n",
    "\n",
    "# Days since\n",
    "from datetime import date\n",
    "df['days_since'] = (date(2025, 10, 22) - df['order_date'].dt.date).dt.days\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- Sales span from early 2024 to late 2025\n",
    "- You should see 7-8 calendar quarters\n",
    "- Most recent orders have days_since near 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf956a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e94307",
   "metadata": {},
   "source": [
    "## Exercise 3B (Advanced): Handling Messy Date Formats\n",
    "**Goal**: Parse dates with multiple formats and handle errors\n",
    "\n",
    "If you want extra practice, create a test dataset with these date variations:\n",
    "```python\n",
    "messy_dates = [\n",
    "    \"2025-03-05\",          # ISO format\n",
    "    \"2025/03/15\",          # Slash format\n",
    "    \"15-03-2025\",          # DD-MM-YYYY\n",
    "    \"March 25, 2025\",      # Text format\n",
    "    \"1719878400\",          # Unix timestamp\n",
    "    \"2025-02-29\",          # Invalid date!\n",
    "    \"Not a date\"\n",
    "]\n",
    "```\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Try multiple formats with coerce for errors\n",
    "df['date_clean'] = pd.to_datetime(df['date_messy'], errors='coerce')\n",
    "\n",
    "# For mixed formats, use infer_datetime_format=True (pandas < 2.0)\n",
    "# or let pandas auto-detect in pandas 2.0+\n",
    "\n",
    "# Unix timestamps need unit specification\n",
    "pd.to_datetime(df['unix_timestamp'], unit='s')\n",
    "\n",
    "# Check for NaT (Not a Time) values\n",
    "df['date_clean'].isna().sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9565b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259ea9d",
   "metadata": {},
   "source": [
    "## Exercise 4: Inner Join of Products with Sales\n",
    "**Goal**: Understand inner joins and identify matched records\n",
    "\n",
    "### Tasks:\n",
    "1. Perform an inner join: sales → products on product_id\n",
    "2. How many sales records are retained? How many are lost?\n",
    "3. Which product_ids appear in sales but NOT in products_master?\n",
    "4. Add product name and category to each sale\n",
    "5. Calculate the discount: (list_price - unit_price) / list_price × 100\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Inner join keeps only matching records\n",
    "merged = sales_df.merge(\n",
    "    products_df, \n",
    "    on='product_id', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Find unmatched sales\n",
    "unmatched = sales_df[~sales_df['product_id'].isin(products_df['product_id'])]\n",
    "\n",
    "# Discount calculation\n",
    "merged['discount_pct'] = (\n",
    "    (merged['list_price'] - merged['unit_price']) / merged['list_price'] * 100\n",
    ")\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- Inner join should drop rows with P-TRI and P-OEM (unknown products)\n",
    "- You'll lose ~20-30 sales records\n",
    "- Products P-LGC and P-BTA won't appear (no sales for them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70518e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9528e77",
   "metadata": {},
   "source": [
    "## Exercise 5: Left Join - Keeping All Sales\n",
    "**Goal**: Preserve all sales records even if products are unknown\n",
    "\n",
    "### Tasks:\n",
    "1. Perform a left join: sales (left) → products (right) on product_id\n",
    "2. Identify sales with missing product information (NaN in product name)\n",
    "3. What's the total revenue from \"unknown\" products?\n",
    "4. Fill missing product names with \"Unknown Product\"\n",
    "5. Create a data quality flag: `is_orphan` = True if product info is missing\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Left join keeps ALL rows from left table\n",
    "merged_left = sales_df.merge(\n",
    "    products_df, \n",
    "    on='product_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Find rows with missing product info\n",
    "orphans = merged_left[merged_left['product'].isna()]\n",
    "\n",
    "# Fill missing values\n",
    "merged_left['product'].fillna('Unknown Product', inplace=True)\n",
    "\n",
    "# Create flag\n",
    "merged_left['is_orphan'] = merged_left['product'].isna()\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- All 260 sales records retained\n",
    "- ~20-30 orphan records (P-TRI and P-OEM)\n",
    "- These orphans represent real revenue that needs investigation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd01c6",
   "metadata": {},
   "source": [
    "## Exercise 6: Right Join - Finding Unsold Products\n",
    "**Goal**: Identify products with no sales history\n",
    "\n",
    "### Tasks:\n",
    "1. Perform a right join: sales (left) → products (right) on product_id\n",
    "2. Which products have no sales? (NaN in order_id)\n",
    "3. What's the total \"potential revenue\" from unsold products' list prices?\n",
    "4. Are discontinued products (status != 'Active') more likely to have no sales?\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Right join keeps ALL rows from right table (products)\n",
    "merged_right = sales_df.merge(\n",
    "    products_df, \n",
    "    on='product_id', \n",
    "    how='right'\n",
    ")\n",
    "\n",
    "# Find products with no sales\n",
    "no_sales = merged_right[merged_right['order_id'].isna()]\n",
    "\n",
    "# Count by status\n",
    "no_sales.groupby('status').size()\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- P-LGC (Legacy, Discontinued) and P-BTA (Beta, Preview) have no sales\n",
    "- These appear with all their product info but NaN sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1780a4",
   "metadata": {},
   "source": [
    "## Exercise 7: Full Outer Join - Complete Picture\n",
    "**Goal**: See ALL products and ALL sales, matched or not\n",
    "\n",
    "### Tasks:\n",
    "1. Perform a full outer join: sales ↔ products on product_id\n",
    "2. Count records in three categories:\n",
    "   - Matched (both sales and product info)\n",
    "   - Sales orphans (sales only)\n",
    "   - Product orphans (products only)\n",
    "3. Calculate what % of revenue comes from \"complete\" records\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Full outer join keeps ALL rows from both tables\n",
    "merged_full = sales_df.merge(\n",
    "    products_df, \n",
    "    on='product_id', \n",
    "    how='outer',\n",
    "    indicator=True  # Adds _merge column!\n",
    ")\n",
    "\n",
    "# Check merge results\n",
    "merged_full['_merge'].value_counts()\n",
    "# 'both': matched records\n",
    "# 'left_only': sales orphans\n",
    "# 'right_only': product orphans\n",
    "\n",
    "# Revenue from complete records\n",
    "complete_revenue = merged_full[\n",
    "    merged_full['_merge'] == 'both'\n",
    "]['revenue'].sum()\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- Total rows > 260 (includes products with no sales)\n",
    "- ~230-240 \"both\" records\n",
    "- ~20-30 \"left_only\" (unknown products)\n",
    "- 2 \"right_only\" (unsold products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ddfd5",
   "metadata": {},
   "source": [
    "## Exercise 8: Region Mapping with Left Join\n",
    "**Goal**: Add region names to sales using the regions dimension\n",
    "\n",
    "### Tasks:\n",
    "1. Join sales → regions_dim on region_code (left join)\n",
    "2. Are there any sales with unmapped region codes? Which ones?\n",
    "3. Create a clean region column that shows the full region name\n",
    "4. Calculate revenue by region (using the full name)\n",
    "5. Identify the \"LATAM\" mystery: this code appears in sales but not in regions_dim\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Join with regions\n",
    "sales_with_regions = sales_df.merge(\n",
    "    regions_df,\n",
    "    on='region_code',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Find unmapped regions\n",
    "unmapped = sales_with_regions[sales_with_regions['region'].isna()]\n",
    "\n",
    "# Group by full region name\n",
    "sales_with_regions.groupby('region')['revenue'].sum()\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- Most regions map correctly\n",
    "- LATAM appears in sales but has no mapping (NaN region name)\n",
    "- ANZ appears in regions_dim but has no sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b5f9a",
   "metadata": {},
   "source": [
    "## Exercise 9: Data Quality Check - Revenue Validation\n",
    "**Goal**: Verify that revenue = units × unit_price\n",
    "\n",
    "### Tasks:\n",
    "1. Calculate expected_revenue = units × unit_price\n",
    "2. Compare expected_revenue to recorded revenue\n",
    "3. Flag records where the difference is > $0.01 (rounding tolerance)\n",
    "4. What % of records have revenue discrepancies?\n",
    "5. Investigate: are discrepancies related to specific products or regions?\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Calculate expected revenue\n",
    "df['expected_revenue'] = df['units'] * df['unit_price']\n",
    "\n",
    "# Compare with tolerance\n",
    "df['revenue_diff'] = abs(df['revenue'] - df['expected_revenue'])\n",
    "df['has_discrepancy'] = df['revenue_diff'] > 0.01\n",
    "\n",
    "# Analysis\n",
    "df['has_discrepancy'].mean() * 100  # % with errors\n",
    "\n",
    "# By product\n",
    "df.groupby('product_id')['has_discrepancy'].mean()\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- Most records should be accurate (expected_revenue ≈ revenue)\n",
    "- Some records will have small rounding differences\n",
    "- Flag any records with large discrepancies (>1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e4005",
   "metadata": {},
   "source": [
    "## Exercise 10: String Cleaning for Better Joins\n",
    "**Goal**: Handle common string matching issues\n",
    "\n",
    "### Tasks:\n",
    "1. Check for whitespace issues: leading/trailing spaces in product_id or region_code\n",
    "2. Check for case inconsistencies (AMER vs amer vs Amer)\n",
    "3. Create cleaned versions of string columns using `.str.strip()` and `.str.upper()`\n",
    "4. Re-run joins with cleaned data and compare results\n",
    "5. Create a function to clean string columns before joining\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Check for whitespace\n",
    "df['product_id'].str.contains('^ | $', regex=True).any()\n",
    "\n",
    "# Clean strings\n",
    "df['product_id_clean'] = df['product_id'].str.strip().str.upper()\n",
    "\n",
    "# Reusable function\n",
    "def clean_string_column(series):\n",
    "    \"\"\"Clean a string column for joining\"\"\"\n",
    "    return series.str.strip().str.upper().str.replace(' +', ' ', regex=True)\n",
    "\n",
    "# Apply before join\n",
    "sales_df['product_id'] = clean_string_column(sales_df['product_id'])\n",
    "products_df['product_id'] = clean_string_column(products_df['product_id'])\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- The current data is already clean, so this won't find issues\n",
    "- But in real data, this cleaning step is ESSENTIAL\n",
    "- 30-50% of failed joins in production are due to whitespace/case issues!\n",
    "\n",
    "### NOTE:\n",
    "**ALWAYS clean string keys before joining**. Can cause joins to silently fail otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69cf3e2",
   "metadata": {},
   "source": [
    "## Exercise 11: Multi-Table Join Chain\n",
    "**Goal**: Combine all three tables in sequence\n",
    "\n",
    "### Tasks:\n",
    "1. Start with sales_df\n",
    "2. Left join → products_master (on product_id)\n",
    "3. Then left join → regions_dim (on region_code)\n",
    "4. Create a comprehensive view with all dimensions\n",
    "5. Calculate revenue by product category and region\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Chain joins\n",
    "result = (sales_df\n",
    "    .merge(products_df, on='product_id', how='left')\n",
    "    .merge(regions_df, on='region_code', how='left')\n",
    ")\n",
    "\n",
    "# Multi-level grouping\n",
    "result.groupby(['category', 'region'])['revenue'].sum().unstack()\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- A table with 260 rows and ~15 columns (all dimensions)\n",
    "- Some NaN values where lookups failed\n",
    "- Pivot table showing category × region performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a10f70",
   "metadata": {},
   "source": [
    "## Exercise 12: Aggregation After Joins (Advanced)\n",
    "**Goal**: Answer complex business questions using joined data\n",
    "\n",
    "### Tasks:\n",
    "1. What's the total revenue for each product category?\n",
    "2. Which region has the highest average discount rate?\n",
    "3. What's the average order size (units) for Core products vs. Add-ons?\n",
    "4. Calculate revenue per product status (Active, Discontinued, Preview)\n",
    "5. Find the month with the highest revenue for each product category\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# After joining all tables...\n",
    "\n",
    "# Revenue by category\n",
    "result.groupby('category')['revenue'].sum()\n",
    "\n",
    "# Average discount by region (from Exercise 4)\n",
    "result['discount_rate'] = (result['list_price'] - result['unit_price']) / result['list_price']\n",
    "result.groupby('region')['discount_rate'].mean()\n",
    "\n",
    "# Time-based aggregation\n",
    "result['month'] = pd.to_datetime(result['order_date']).dt.to_period('M')\n",
    "result.groupby(['category', 'month'])['revenue'].sum().groupby(level=0).idxmax()\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- Core category should dominate revenue\n",
    "- Discount rates vary by region\n",
    "- Clear seasonal patterns may emerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f953ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d0ada1",
   "metadata": {},
   "source": [
    "## Exercise 13: Pivot Tables & Cross-Tabs\n",
    "**Goal**: Create Excel-style pivot tables in pandas\n",
    "\n",
    "### Tasks:\n",
    "1. Create a pivot table: products (rows) × regions (columns), values = total revenue\n",
    "2. Create another pivot: months (rows) × products (columns), values = units sold\n",
    "3. Calculate percentage of total revenue by product within each region\n",
    "4. Find which product-region combination has the highest average order value\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Pivot table\n",
    "pivot_revenue = result.pivot_table(\n",
    "    index='product',\n",
    "    columns='region',\n",
    "    values='revenue',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Percentage of total\n",
    "pivot_pct = pivot_revenue.div(pivot_revenue.sum(axis=0), axis=1) * 100\n",
    "\n",
    "# Cross-tab (alternative)\n",
    "pd.crosstab(result['product'], result['region'], values=result['revenue'], aggfunc='sum')\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- A matrix showing revenue distribution\n",
    "- Empty cells (0 or NaN) where product-region combinations don't exist\n",
    "- Clear winners and losers by segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b15eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861eaaa9",
   "metadata": {},
   "source": [
    "## Exercise 14: Unpivot (Melt) for Reshaping\n",
    "**Goal**: Convert wide-format data back to long format\n",
    "\n",
    "### Tasks:\n",
    "1. Take the pivot table from Exercise 13 (products × regions)\n",
    "2. Unpivot it back to long format using `.melt()`\n",
    "3. Verify the total revenue matches the original\n",
    "4. Filter to show only non-zero combinations\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Melt (unpivot)\n",
    "long_format = pivot_revenue.reset_index().melt(\n",
    "    id_vars='product',\n",
    "    var_name='region',\n",
    "    value_name='revenue'\n",
    ")\n",
    "\n",
    "# Remove zeros\n",
    "long_format = long_format[long_format['revenue'] > 0]\n",
    "\n",
    "# Verify totals\n",
    "print(f\"Original: {result['revenue'].sum():.2f}\")\n",
    "print(f\"After pivot/melt: {long_format['revenue'].sum():.2f}\")\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- Long format with ~150-180 non-zero product-region combinations\n",
    "- Total revenue should match original\n",
    "\n",
    "### When to Use:\n",
    "- Pivot: For analysis and visualization (wide format)\n",
    "- Melt: For database loading or further processing (long format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b6d01d",
   "metadata": {},
   "source": [
    "## Exercise 15: Finding Duplicates & Near-Duplicates\n",
    "**Goal**: Identify and handle duplicate records\n",
    "\n",
    "### Tasks:\n",
    "1. Check for exact duplicates across all columns\n",
    "2. Check for duplicates based only on order_id + product_id (same order, same product)\n",
    "3. Find \"near-duplicates\": same order_id, product_id, units, but different revenue\n",
    "4. Create a strategy to handle duplicates:\n",
    "   - Keep first occurrence?\n",
    "   - Keep highest revenue?\n",
    "   - Flag for manual review?\n",
    "\n",
    "### Hints:\n",
    "```python\n",
    "# Exact duplicates\n",
    "duplicates = df[df.duplicated(keep=False)]\n",
    "\n",
    "# Duplicates on specific columns\n",
    "order_dupes = df[df.duplicated(subset=['order_id', 'product_id'], keep=False)]\n",
    "\n",
    "# Find near-duplicates (same order/product, different revenue)\n",
    "near_dupes = df.groupby(['order_id', 'product_id']).filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Remove duplicates\n",
    "df_deduped = df.drop_duplicates(subset=['order_id', 'product_id'], keep='first')\n",
    "\n",
    "# Or keep the one with highest revenue\n",
    "df_deduped = df.sort_values('revenue', ascending=False).drop_duplicates(\n",
    "    subset=['order_id', 'product_id'], \n",
    "    keep='first'\n",
    ")\n",
    "```\n",
    "\n",
    "### Expected Output:\n",
    "- The current dataset is clean (no duplicates)\n",
    "- But know how to find them!\n",
    "- In real data, 2-5% of rows are often duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf422fb",
   "metadata": {},
   "source": [
    "## Bonus Exercise: Create a Reusable Data Cleaning Pipeline\n",
    "**Goal**: Build a function that cleans and joins all data\n",
    "\n",
    "### Task:\n",
    "Create a function that:\n",
    "1. Loads all three CSVs\n",
    "2. Cleans string columns (strip, uppercase)\n",
    "3. Parses dates\n",
    "4. Performs all joins\n",
    "5. Adds calculated fields (discount, margin, flags)\n",
    "6. Returns a clean, analysis-ready DataFrame\n",
    "\n",
    "### Template:\n",
    "```python\n",
    "def prepare_sales_data(sales_path, products_path, regions_path):\n",
    "    \"\"\"\n",
    "    Load and prepare sales data for analysis\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Clean, joined, analysis-ready data\n",
    "    \"\"\"\n",
    "    # Load\n",
    "    sales = pd.read_csv(sales_path)\n",
    "    products = pd.read_csv(products_path)\n",
    "    regions = pd.read_csv(regions_path)\n",
    "    \n",
    "    # Clean strings\n",
    "    for df in [sales, products, regions]:\n",
    "        for col in df.select_dtypes(include='object').columns:\n",
    "            df[col] = df[col].str.strip().str.upper()\n",
    "    \n",
    "    # Parse dates\n",
    "    sales['order_date'] = pd.to_datetime(sales['order_date'])\n",
    "    \n",
    "    # Join\n",
    "    result = (sales\n",
    "        .merge(products, on='product_id', how='left')\n",
    "        .merge(regions, on='region_code', how='left')\n",
    "    )\n",
    "    \n",
    "    # Add calculated fields\n",
    "    result['discount_pct'] = ((result['list_price'] - result['unit_price']) \n",
    "                               / result['list_price'] * 100)\n",
    "    result['is_orphan'] = result['product'].isna()\n",
    "    \n",
    "    # Add time dimensions\n",
    "    result['year'] = result['order_date'].dt.year\n",
    "    result['quarter'] = result['order_date'].dt.quarter\n",
    "    result['month'] = result['order_date'].dt.month\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Use it\n",
    "df = prepare_sales_data('sales_records.csv', 'products_master.csv', 'regions_dim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobihouse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
